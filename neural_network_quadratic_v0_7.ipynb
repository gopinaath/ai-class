{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Networks: Learn a Quadratic Function from Dataset\n",
        "\n",
        "This notebook teaches a neural network to learn a quadratic function using a pre-generated CSV dataset.\n",
        "\n",
        "We'll:\n",
        "- Load training and test datasets from CSV files\n",
        "- Build a non-linear neural network model\n",
        "- Train using the dataset\n",
        "- Evaluate performance\n",
        "\n",
        "**Target Function**: `f(a, b) = 2a² + 3b + 1`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "print(torch.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the dataset from GitHub\n",
        "We'll load the pre-generated quadratic datasets from the GitHub repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets from GitHub repository\n",
        "import requests\n",
        "import io\n",
        "import os\n",
        "\n",
        "# GitHub repository URLs for the CSV files\n",
        "github_base_url = \"https://raw.githubusercontent.com/gopinaath/ai-class/main/\"\n",
        "train_url = github_base_url + \"quadratic_train.csv\"\n",
        "test_url = github_base_url + \"quadratic_test.csv\"\n",
        "\n",
        "print(\"Loading datasets from GitHub repository...\")\n",
        "print(f\"Training data URL: {train_url}\")\n",
        "print(f\"Test data URL: {test_url}\")\n",
        "\n",
        "try:\n",
        "    # Download and load training data\n",
        "    train_response = requests.get(train_url)\n",
        "    train_response.raise_for_status()  # Raise an exception for bad status codes\n",
        "    train_df = pd.read_csv(io.StringIO(train_response.text))\n",
        "\n",
        "    # Download and load test data\n",
        "    test_response = requests.get(test_url)\n",
        "    test_response.raise_for_status()\n",
        "    test_df = pd.read_csv(io.StringIO(test_response.text))\n",
        "\n",
        "    print(\"✅ Datasets loaded successfully from GitHub!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to load from GitHub: {e}\")\n",
        "    print(\"🔄 Falling back to local dataset generation...\")\n",
        "    \n",
        "    # Fallback: generate datasets locally\n",
        "    if not os.path.exists('quadratic_train.csv'):\n",
        "        exec(open('generate_quadratic_dataset.py').read())\n",
        "    \n",
        "    train_df = pd.read_csv('quadratic_train.csv')\n",
        "    test_df = pd.read_csv('quadratic_test.csv')\n",
        "    print(\"✅ Local datasets loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis\n",
        "Let's explore our dataset to understand what we're working with.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to PyTorch tensors\n",
        "train_inputs = torch.tensor(train_df[['a', 'b']].values, dtype=torch.float32)\n",
        "train_targets = torch.tensor(train_df['target'].values, dtype=torch.float32).unsqueeze(1)\n",
        "test_inputs = torch.tensor(test_df[['a', 'b']].values, dtype=torch.float32)\n",
        "test_targets = torch.tensor(test_df['target'].values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "print(f\"Training data shape: {train_inputs.shape}\")\n",
        "print(f\"Test data shape: {test_inputs.shape}\")\n",
        "print(f\"First few training examples:\")\n",
        "print(train_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"=== Dataset Overview ===\")\n",
        "print(f\"Training samples: {len(train_df)}\")\n",
        "print(f\"Test samples: {len(test_df)}\")\n",
        "print(f\"Features: {list(train_df.columns[:-1])}\")  # All columns except 'target'\n",
        "print(f\"Target: {train_df.columns[-1]}\")\n",
        "\n",
        "print(\"\\n=== Training Data Statistics ===\")\n",
        "print(train_df.describe())\n",
        "\n",
        "print(\"\\n=== Test Data Statistics ===\")\n",
        "print(test_df.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the data\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# 1. Distribution of inputs\n",
        "axes[0, 0].hist(train_df['a'], bins=30, alpha=0.7, label='a', color='blue')\n",
        "axes[0, 0].hist(train_df['b'], bins=30, alpha=0.7, label='b', color='red')\n",
        "axes[0, 0].set_title('Distribution of Input Features')\n",
        "axes[0, 0].set_xlabel('Value')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Distribution of target\n",
        "axes[0, 1].hist(train_df['target'], bins=30, alpha=0.7, color='green')\n",
        "axes[0, 1].set_title('Distribution of Target Values')\n",
        "axes[0, 1].set_xlabel('Target Value')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Scatter plot: a vs target\n",
        "axes[1, 0].scatter(train_df['a'], train_df['target'], alpha=0.6, s=10)\n",
        "axes[1, 0].set_title('Feature a vs Target')\n",
        "axes[1, 0].set_xlabel('Feature a')\n",
        "axes[1, 0].set_ylabel('Target')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Scatter plot: b vs target\n",
        "axes[1, 1].scatter(train_df['b'], train_df['target'], alpha=0.6, s=10, color='red')\n",
        "axes[1, 1].set_title('Feature b vs Target')\n",
        "axes[1, 1].set_xlabel('Feature b')\n",
        "axes[1, 1].set_ylabel('Target')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Insights from EDA\n",
        "\n",
        "**What we learned:**\n",
        "- **Input range**: Both `a` and `b` are uniformly distributed between -2 and 2\n",
        "- **Target range**: Target values range from ~-3 to ~9 (since min is 2×(-2)² + 3×(-2) + 1 = 3, max is 2×2² + 3×2 + 1 = 15)\n",
        "- **Non-linear relationship**: We can see clear quadratic patterns in the scatter plots\n",
        "- **Correlations**: \n",
        "  - `a` has strong positive correlation with target (due to a² term)\n",
        "  - `b` has moderate positive correlation with target (linear term)\n",
        "\n",
        "**Why this needs a non-linear model:**\n",
        "- The relationship involves a² (quadratic term), which requires non-linear activation functions\n",
        "- A simple linear model cannot capture the quadratic relationship\n",
        "- Multiple hidden layers with ReLU can approximate this function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the model\n",
        "We need a non-linear model with multiple layers to learn the quadratic function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 20),    # Input layer: 2 inputs -> 20 hidden neurons\n",
        "    nn.ReLU(),           # Non-linear activation\n",
        "    nn.Linear(20, 20),   # Hidden layer: 20 -> 20\n",
        "    nn.ReLU(),           # Non-linear activation\n",
        "    nn.Linear(20, 10),   # Hidden layer: 20 -> 10\n",
        "    nn.ReLU(),           # Non-linear activation\n",
        "    nn.Linear(10, 1)     # Output layer: 10 -> 1 output\n",
        ")\n",
        "model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model\n",
        "We'll train using the entire dataset with mini-batches and Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_history = []\n",
        "\n",
        "batch_size = 32\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    # Process data in mini-batches\n",
        "    for i in range(0, len(train_inputs), batch_size):\n",
        "        batch_inputs = train_inputs[i:i+batch_size]\n",
        "        batch_targets = train_targets[i:i+batch_size]\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(batch_inputs)\n",
        "        loss = criterion(outputs, batch_targets)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        loss_history.append(loss.item())\n",
        "    \n",
        "    avg_loss = epoch_loss / num_batches\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.6f}\")\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(loss_history)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# Plot every 10th point to reduce noise\n",
        "plt.plot(loss_history[::10])\n",
        "plt.title('Training Loss (Every 10th Batch)')\n",
        "plt.xlabel('Batch (x10)')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the model\n",
        "Test the model on the test dataset and some specific examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on the test dataset\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(test_inputs)\n",
        "    test_loss = criterion(test_outputs, test_targets)\n",
        "    \n",
        "print(f\"Test Loss: {test_loss.item():.6f}\")\n",
        "\n",
        "# Test on specific examples\n",
        "test_cases = [(1.0, 1.0), (2.0, -1.0), (0.5, 0.5), (-1.0, 2.0)]\n",
        "\n",
        "print(\"\\nTesting on specific examples:\")\n",
        "print(\"Input (a, b) | Prediction | Expected | Error\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for a, b in test_cases:\n",
        "    pred = model(torch.tensor([a, b], dtype=torch.float32))\n",
        "    expected = 2*a**2 + 3*b + 1  # The actual quadratic function\n",
        "    error = abs(pred.item() - expected)\n",
        "    print(f\"({a:4.1f}, {b:4.1f})    | {pred.item():8.3f} | {expected:7.3f} | {error:.3f}\")\n",
        "\n",
        "print(f\"\\nExpected function: f(a, b) = 2a² + 3b + 1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional evaluation: Test on edge cases\n",
        "print(\"\\n=== Additional Test Cases ===\")\n",
        "edge_cases = [(0, 0), (-2, -2), (2, 2), (0, -2), (2, 0)]\n",
        "\n",
        "print(\"Edge cases:\")\n",
        "print(\"Input (a, b) | Prediction | Expected | Error\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for a, b in edge_cases:\n",
        "    pred = model(torch.tensor([a, b], dtype=torch.float32))\n",
        "    expected = 2*a**2 + 3*b + 1\n",
        "    error = abs(pred.item() - expected)\n",
        "    print(f\"({a:4.1f}, {b:4.1f})    | {pred.item():8.3f} | {expected:7.3f} | {error:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture Summary\n",
        "\n",
        "**Network Structure:**\n",
        "- **Input Layer**: 2 neurons (features a, b)\n",
        "- **Hidden Layer 1**: 20 neurons with ReLU activation\n",
        "- **Hidden Layer 2**: 20 neurons with ReLU activation  \n",
        "- **Hidden Layer 3**: 10 neurons with ReLU activation\n",
        "- **Output Layer**: 1 neuron (predicted target)\n",
        "\n",
        "**Why this architecture works:**\n",
        "- **Multiple hidden layers** allow the network to learn complex non-linear patterns\n",
        "- **ReLU activations** enable the network to approximate the quadratic function\n",
        "- **Sufficient neurons** provide enough capacity to learn the relationship\n",
        "- **Adam optimizer** helps with convergence on this non-linear problem\n",
        "\n",
        "**Key differences from linear model:**\n",
        "- Requires non-linear activations (ReLU)\n",
        "- Needs multiple layers to capture quadratic relationship\n",
        "- More complex training process due to non-linearity\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
